diff --git a/kernel/sched/mod/Makefile b/kernel/sched/mod/Makefile
index 2c617d7..5f78875 100644
--- a/kernel/sched/mod/Makefile
+++ b/kernel/sched/mod/Makefile
@@ -15,9 +15,9 @@ CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
 objs-y += core.o
-objs-y += idle.o fair.o rt.o deadline.o
+objs-y += idle_task.o fair.o rt.o deadline.o
 
-objs-$(CONFIG_SMP) += cpupri.o cpudeadline.o topology.o stop_task.o pelt.o
+objs-$(CONFIG_SMP) += cpupri.o cpudeadline.o stop_task.o
 objs-$(CONFIG_SCHEDSTATS) += stats.o
 objs-$(CONFIG_SCHED_DEBUG) += debug.o
 
diff --git a/kernel/sched/mod/core.c b/kernel/sched/mod/core.c
index 933ec23..c15d604 100644
--- a/kernel/sched/mod/core.c
+++ b/kernel/sched/mod/core.c
@@ -57,6 +57,9 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 #include <linux/sysctl.h>
+
+#undef CONFIG_FTRACE_SYSCALLS
+
 #include <linux/syscalls.h>
 #include <linux/times.h>
 #include <linux/tsacct_kern.h>
@@ -89,7 +92,6 @@
 #include "../../workqueue_internal.h"
 #include "../../smpboot.h"
 
-#define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
 
 #ifdef smp_mb__before_atomic
@@ -7533,9 +7535,13 @@ void sched_cpu_deactivate(unsigned int cpu);
 #else
 #endif /* CONFIG_SMP */
 
+extern char __module_sched_start[], __module_sched_end[];
+
 int in_sched_functions(unsigned long addr)
 {
 	return in_lock_functions(addr) ||
+		(addr >= (unsigned long)__module_sched_start
+		&& addr < (unsigned long)__module_sched_end) ||
 		(addr >= (unsigned long)__sched_text_start
 		&& addr < (unsigned long)__sched_text_end);
 }
diff --git a/kernel/sched/mod/main.c b/kernel/sched/mod/main.c
index 4730740..5adc57a 100644
--- a/kernel/sched/mod/main.c
+++ b/kernel/sched/mod/main.c
@@ -12,6 +12,8 @@
 #include <linux/sched/task.h>
 #include <linux/sysfs.h>
 #include <linux/version.h>
+#include <linux/debugfs.h>
+#include <linux/proc_fs.h>
 #include "sched.h"
 #include "mempool.h"
 #include "head_jump.h"
@@ -19,7 +21,20 @@
 
 #define MAX_CPU_NR		1024
 
-extern void __orig___schedule(bool);
+#define smp_cond_load_relaxed(ptr, cond_expr) ({		\
+	typeof(ptr) __PTR = (ptr);			      \
+	typeof(*ptr) VAL;					\
+	for (;;) {					      \
+		VAL = READ_ONCE(*__PTR);			\
+		if (cond_expr)				  \
+			break;				  \
+		cpu_relax();				    \
+	}							\
+	VAL;						    \
+})
+#define atomic_cond_read_relaxed(v, c) smp_cond_load_relaxed(&(v)->counter, (c))
+
+extern void __orig___schedule(void);
 int process_id[MAX_CPU_NR];
 atomic_t cpu_finished;
 atomic_t clear_finished;
@@ -48,10 +63,9 @@ extern struct percpu_rw_semaphore cpuset_rwsem;
 	percpu_up_write(&cpuset_rwsem)
 #endif
 
-extern cpumask_var_t sd_sysctl_cpus;
 extern const struct file_operations __mod_sched_feat_fops;
-extern const struct seq_operations __mod_sched_debug_sops;
-extern const struct seq_operations __mod_schedstat_sops;
+extern const struct file_operations __mod_sched_debug_fops;
+extern const struct file_operations __mod_proc_schedstat_operations;
 
 static struct dentry *sched_features_dir;
 static s64 stop_time;
@@ -260,8 +274,25 @@ static int sync_sched_mod(void *func)
 }
 
 #ifdef CONFIG_SCHED_DEBUG
+extern void __mod_register_sched_domain_sysctl(void);
+extern void __mod_unregister_sched_domain_sysctl(void);
+
+extern struct ctl_table_header *__orig_sd_sysctl_header;
+extern struct ctl_table __orig_sd_ctl_dir[];
+extern void __orig_sd_free_ctl_entry(struct ctl_table **tablep);
+
 extern void __orig_register_sched_domain_sysctl(void);
-extern void __orig_unregister_sched_domain_sysctl(void);
+static void __orig_unregister_sched_domain_sysctl(void)
+{
+	if (__orig_sd_sysctl_header)
+		unregister_sysctl_table(__orig_sd_sysctl_header);
+
+	__orig_sd_sysctl_header = NULL;
+
+	if (__orig_sd_ctl_dir[0].child)
+		__orig_sd_free_ctl_entry(&__orig_sd_ctl_dir[0].child);
+}
+/* DON'T MODIFY INLINE EXTERNAL FUNCTION unregister_sched_domain_sysctl */
 
 static inline void install_sched_domain_sysctl(void)
 {
@@ -269,7 +300,7 @@ static inline void install_sched_domain_sysctl(void)
 	plugsched_cpuset_lock();
 
 	__orig_unregister_sched_domain_sysctl();
-	register_sched_domain_sysctl();
+	__mod_register_sched_domain_sysctl();
 
 	plugsched_cpuset_unlock();
 	mutex_unlock(&cgroup_mutex);
@@ -280,8 +311,7 @@ static inline void restore_sched_domain_sysctl(void)
 	mutex_lock(&cgroup_mutex);
 	plugsched_cpuset_lock();
 
-	unregister_sched_domain_sysctl();
-	cpumask_copy(sd_sysctl_cpus, cpu_possible_mask);
+	__mod_unregister_sched_domain_sysctl();
 	__orig_register_sched_domain_sysctl();
 
 	plugsched_cpuset_unlock();
@@ -307,7 +337,7 @@ void install_sched_debugfs(void)
 }
 
 extern struct file_operations __orig_sched_feat_fops;
-extern struct seq_operations  __orig_sched_debug_sops;
+extern struct file_operations __orig_sched_debug_fops;
 
 void restore_sched_debugfs(void)
 {
@@ -320,7 +350,7 @@ int install_sched_debug_procfs(void)
 {
 	remove_proc_entry("sched_debug", NULL);
 
-	if (!proc_create_seq("sched_debug", 0444, NULL, &__mod_sched_debug_sops))
+	if (!proc_create("sched_debug", 0444, NULL, &__mod_sched_debug_fops))
 		return -ENOMEM;
 
 	return 0;
@@ -330,7 +360,7 @@ int restore_sched_debug_procfs(void)
 {
 	remove_proc_entry("sched_debug", NULL);
 
-	if (!proc_create_seq("sched_debug", 0444, NULL, &__orig_sched_debug_sops))
+	if (!proc_create("sched_debug", 0444, NULL, &__orig_sched_debug_fops))
 		return -ENOMEM;
 
 	return 0;
@@ -338,14 +368,14 @@ int restore_sched_debug_procfs(void)
 #endif
 
 #ifdef CONFIG_SCHEDSTATS
-extern struct seq_operations __orig_schedstat_sops;
+extern struct file_operations __orig_proc_schedstat_operations;
 
 /* schedstat interface in proc */
 int install_proc_schedstat(void)
 {
 	remove_proc_entry("schedstat", NULL);
 
-	if (!proc_create_seq("schedstat", 0444, NULL, &__mod_schedstat_sops))
+	if (!proc_create("schedstat", 0444, NULL, &__mod_proc_schedstat_operations))
 		return -ENOMEM;
 
 	return 0;
@@ -355,7 +385,7 @@ int restore_proc_schedstat(void)
 {
 	remove_proc_entry("schedstat", NULL);
 
-	if (!proc_create_seq("schedstat", 0444, NULL, &__orig_schedstat_sops))
+	if (!proc_create("schedstat", 0444, NULL, &__orig_proc_schedstat_operations))
 		return -ENOMEM;
 
 	return 0;
diff --git a/kernel/sched/mod/sched_rebuild.c b/kernel/sched/mod/sched_rebuild.c
index 6e8a4c7..94889e9 100644
--- a/kernel/sched/mod/sched_rebuild.c
+++ b/kernel/sched/mod/sched_rebuild.c
@@ -6,8 +6,6 @@
 #include <linux/sched.h>
 #include "sched.h"
 
-extern void __orig_set_rq_offline(struct rq*);
-extern void __orig_set_rq_online(struct rq*);
 extern unsigned int process_id[];
 
 extern struct sched_class __orig_stop_sched_class;
@@ -40,6 +38,40 @@ struct sched_class *mod_class[] = {
 #define NR_SCHED_CLASS 5
 struct sched_class bak_class[NR_SCHED_CLASS];
 
+extern void __mod_set_rq_offline(struct rq*);
+extern void __mod_set_rq_online(struct rq*);
+
+static void __orig_set_rq_online(struct rq *rq)
+{
+	if (!rq->online) {
+		const struct sched_class *class;
+
+		cpumask_set_cpu(rq->cpu, rq->rd->online);
+		rq->online = 1;
+
+		for_each_class(class) {
+			if (class->rq_online)
+				class->rq_online(rq);
+		}
+	}
+}
+/* DON'T MODIFY INLINE EXTERNAL FUNCTION __orig_set_rq_online */
+
+static void __orig_set_rq_offline(struct rq *rq)
+{
+	if (rq->online) {
+		const struct sched_class *class;
+
+		for_each_class(class) {
+			if (class->rq_offline)
+				class->rq_offline(rq);
+		}
+
+		cpumask_clear_cpu(rq->cpu, rq->rd->online);
+		rq->online = 0;
+	}
+}
+/* DON'T MODIFY INLINE EXTERNAL FUNCTION __orig_set_rq_offline */
 
 static inline void do_write_cr0(unsigned long val)
 {
@@ -70,11 +102,11 @@ void clear_sched_state(bool mod)
 {
 	struct task_struct *g, *p;
 	struct rq *rq = this_rq();
-	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK;
+	int queue_flags = DEQUEUE_SAVE;
 
 	raw_spin_lock(&rq->lock);
 	if (mod) {
-		set_rq_offline(rq);
+		__mod_set_rq_offline(rq);
 	} else {
 		__orig_set_rq_offline(rq);
 	}
@@ -86,9 +118,6 @@ void clear_sched_state(bool mod)
 		if (p == rq->stop)
 			continue;
 
-		/* To avoid SCHED_WARN_ON(rq->clock_update_flags < RQCF_ACT_SKIP) */
-		rq->clock_update_flags = RQCF_UPDATED;
-
 		if (task_on_rq_queued(p))
 			p->sched_class->dequeue_task(rq, p, queue_flags);
 	}
@@ -100,12 +129,12 @@ void rebuild_sched_state(bool mod)
 	struct task_struct *g, *p;
 	struct task_group *tg;
 	struct rq *rq = this_rq();
-	int queue_flags = ENQUEUE_RESTORE | ENQUEUE_MOVE | ENQUEUE_NOCLOCK;
+	int queue_flags = ENQUEUE_RESTORE;
 	int cpu = smp_processor_id();
 
 	raw_spin_lock(&rq->lock);
 	if (mod) {
-		set_rq_online(rq);
+		__mod_set_rq_online(rq);
 	} else {
 		__orig_set_rq_online(rq);
 	}
@@ -130,12 +159,12 @@ void rebuild_sched_state(bool mod)
 		if (tg == &root_task_group)
 			continue;
 
-		if (tg->cfs_bandwidth.period_active) {
+		if (hrtimer_active(&tg->cfs_bandwidth.period_timer)) {
 			hrtimer_restart(&tg->cfs_bandwidth.period_timer);
 			hrtimer_restart(&tg->cfs_bandwidth.slack_timer);
 		}
 #ifdef CONFIG_RT_GROUP_SCHED
-		if (tg->rt_bandwidth.rt_period_active)
+		if (hrtimer_active(&tg->rt_bandwidth.rt_period_timer))
 			hrtimer_restart(&tg->rt_bandwidth.rt_period_timer);
 #endif
 	}
